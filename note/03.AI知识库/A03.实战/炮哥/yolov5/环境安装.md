

```shell

conda create -n pao_v5  python==3.8
conda activate pao_v5

pip install labelimg

conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=11.3 -c pytorch -c conda-forge

conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.6 -c pytorch -c conda-forge


pip install -r coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow


# 模型检测
/home/ros/anaconda3/envs/yolov5/bin/python detect.py --source source_files/hardhat.mp4

# onnx模型导出
/home/ros/anaconda3/envs/yolov5/bin/python export.py --weights  runs/train/exp5/weights/best.pt --include onnx --img 640

python -m onnxsim input_model.onnx output_model.onnx


```



如何导出yolov5的3个检测头
![yolov5导出3个检测头](figture/Pasted%20image%2020241208215417.png)

```python
def forward(self, x):
        for i in range(self.nl):
            x[i] = self.m[i](x[i])
        return x[0],x[1],x[2]
        # z = []  # inference output
        # for i in range(self.nl):
        #     x[i] = self.m[i](x[i])  # conv
        #     bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
        #     x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()

        #     if not self.training:  # inference
        #         if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:
        #             self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)

        #         y = x[i].sigmoid()
        #         if self.inplace:
        #             y[..., 0:2] = (y[..., 0:2] * 2 + self.grid[i]) * self.stride[i]  # xy
        #             y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
        #         else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953
        #             xy, wh, conf = y.split((2, 2, self.nc + 1), 4)  # y.tensor_split((2, 4, 5), 4)  # torch 1.8.0
        #             xy = (xy * 2 + self.grid[i]) * self.stride[i]  # xy
        #             wh = (wh * 2) ** 2 * self.anchor_grid[i]  # wh
        #             y = torch.cat((xy, wh, conf), 4)
        #         z.append(y.view(bs, -1, self.no))
        
        # return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x)
```

参考：
https://www.bilibili.com/video/BV1XL17YREJo/?vd_source=3f7e86d544ccf5408190e0294ca8ec1a

https://gitcode.csdn.net/65ec50551a836825ed797fb2.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6MTAyNjM3MSwiZXhwIjoxNzM0MjY3MTM5LCJpYXQiOjE3MzM2NjIzMzksInVzZXJuYW1lIjoiTGVvbmlkYXMyMDE5In0.yjE2wGFbS2Ofs6d-e9jZNnoDrMBS9sGmH7Dl1fSwCqY&spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Eactivity-1-131980324-blog-143301422.235%5Ev43%5Epc_blog_bottom_relevance_base1&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Eactivity-1-131980324-blog-143301422.235%5Ev43%5Epc_blog_bottom_relevance_base1&utm_relevant_index=1


https://blog.csdn.net/apchy_ll/article/details/143301422

https://blog.csdn.net/weixin_38252409/article/details/135585593?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-2-135585593-blog-143301422.235%5Ev43%5Epc_blog_bottom_relevance_base1&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-2-135585593-blog-143301422.235%5Ev43%5Epc_blog_bottom_relevance_base1&utm_relevant_index=3


导出4个检测头
```python
def forward(self, x):
        k = []  
        z = []  # inference output
        print("===> nl: ", self.nl)
        for i in range(self.nl):
            x[i] = self.m[i](x[i])  # conv
            k.append(x[i])
            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
        # return k[0], k[1], k[2]
            if not self.training:  # inference
                if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:
                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)

                y = x[i].sigmoid()
                if self.inplace:
                    y[..., 0:2] = (y[..., 0:2] * 2 + self.grid[i]) * self.stride[i]  # xy
                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953
                    xy, wh, conf = y.split((2, 2, self.nc + 1), 4)  # y.tensor_split((2, 4, 5), 4)  # torch 1.8.0
                    xy = (xy * 2 + self.grid[i]) * self.stride[i]  # xy
                    wh = (wh * 2) ** 2 * self.anchor_grid[i]  # wh
                    y = torch.cat((xy, wh, conf), 4)
                z.append(y.view(bs, -1, self.no))
        
        # return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x), k[0], k[1], k[2]
        return torch.cat(z, 1), k[0], k[1], k[2]
```

导出3个检测头

![](figture/Pasted%20image%2020241212220256.png)


```
1 测试情况
（1）最新QAT版本模型办公椅误检复现概率还是比较大，且误检置信度比较高（0.72）；
（2）结合移动侦测之后，可以过滤掉一些办公椅误检，同时会引入漏检增加（如人坐在椅子上转动时也会存在较多漏检）

误检情况可以调整阈值过滤掉，可以对齐君正的效果，但漏检情况会变多，整体体验效果上会比君正差。

2 当前人形结合移动侦测处理逻辑是：
1、人形检测框 会 和移动区域框做交集，计算求得分，得分大于0，则认为是移动区域检测到人形（置信度大于0.3，可调整）；
2、如果为静止检测区域，但人形置信度大于0.65（高置信度，可调整），则也会认为是人形，进行画框；

3 后续改进
（1）应用端继续实验，调整移动侦测灵敏度和阈值，减少漏检情况


```